{"cells":[{"cell_type":"markdown","source":["Project Of RIO125 - Automate detection and recognition of grammatical error.\n","Devloped By :- Vishwajeet Vijay Bhosale"],"metadata":{"id":"L3L9C4mird_h"},"id":"L3L9C4mird_h"},{"cell_type":"code","execution_count":1,"id":"127a621c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"127a621c","executionInfo":{"status":"ok","timestamp":1713079469938,"user_tz":-330,"elapsed":17616,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}},"outputId":"cf402ae2-9b29-44bc-d944-a7a06cd4e2c8"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import pandas as pd\n","import re\n","import string\n","from sklearn.model_selection import train_test_split\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","import nltk\n","nltk.download('punkt')\n","\n","\n","# Load the dataset\n","df = pd.read_csv(\"/content/sample_data/input_data.csv\")  # Replace \"your_dataset.csv\" with the path to your dataset\n","\n","# Data Preprocessing\n","def preprocess_text(text):\n","    # Lowercase the text\n","    text = text.lower()\n","\n","    # Remove punctuation\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","    # Tokenization\n","    tokens = word_tokenize(text)\n","\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    # Join tokens back into a string\n","    preprocessed_text = ' '.join(tokens)\n","\n","    return preprocessed_text\n","\n","# Apply preprocessing to the text column\n","df['processed_text'] = df['text'].apply(preprocess_text)\n","\n","# Split the dataset into training, validation, and testing sets\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n","\n","# Save preprocessed datasets\n","train_df.to_csv(\"train_dataset.csv\", index=False)\n","val_df.to_csv(\"val_dataset.csv\", index=False)\n","test_df.to_csv(\"test_dataset.csv\", index=False)\n"]},{"cell_type":"code","execution_count":2,"id":"d4fd26b6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4fd26b6","executionInfo":{"status":"ok","timestamp":1713079481819,"user_tz":-330,"elapsed":8219,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}},"outputId":"e52a440a-0da8-44d6-e6a5-9ac6b7ce949c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Head of Train Dataset:\n","                                                text  label  \\\n","0                 I'll write something about myself.      1   \n","1      To use English well, I register on this site.      1   \n","2  I don't know how much money it would cost, or ...      1   \n","3                 because it is a very good website.      1   \n","4  KanJi is japanese character and have variety o...      0   \n","\n","                                      processed_text  \n","0                                ill write something  \n","1                     use english well register site  \n","2  dont know much money would cost much time woul...  \n","3                                       good website  \n","4           kanji japanese character variety meaning  \n","\n","Head of Validation Dataset:\n","                                                text  label  \\\n","0                               I'm college student.      0   \n","1                             be hired as consultant      0   \n","2                                It’s sunny morning.      0   \n","3         I was looking forward to these few months.      1   \n","4  I asked class teachers to plant seeds in the b...      0   \n","\n","                             processed_text  \n","0                        im college student  \n","1                          hired consultant  \n","2                           ’ sunny morning  \n","3                     looking forward month  \n","4  asked class teacher plant seed beginning  \n","\n","Head of Test Dataset:\n","                                                text  label  \\\n","0            2-[His mother words touched his heart].      0   \n","1                                         many time.      0   \n","2                    Sometime l want change my work.      0   \n","3  I think that I may hold a fine if there is a p...      0   \n","4  I get lazy, which is the most serious problem ...      1   \n","\n","                            processed_text  \n","0           2his mother word touched heart  \n","1                                many time  \n","2              sometime l want change work  \n","3  think may hold fine person able protect  \n","4            get lazy serious problem mine  \n"]}],"source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Load preprocessed datasets\n","train_df = pd.read_csv(\"train_dataset.csv\")\n","val_df = pd.read_csv(\"val_dataset.csv\")\n","test_df = pd.read_csv(\"test_dataset.csv\")\n","\n","# Handle missing values\n","train_df.dropna(subset=['processed_text'], inplace=True)\n","val_df.dropna(subset=['processed_text'], inplace=True)\n","test_df.dropna(subset=['processed_text'], inplace=True)\n","\n","# Print the head of updated datasets\n","print(\"Head of Train Dataset:\")\n","print(train_df.head())\n","\n","print(\"\\nHead of Validation Dataset:\")\n","print(val_df.head())\n","\n","print(\"\\nHead of Test Dataset:\")\n","print(test_df.head())\n","\n","# Feature Engineering\n","def extract_features(df):\n","    # TF-IDF Vectorization\n","    tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n","    tfidf_features = tfidf_vectorizer.fit_transform(df['processed_text']).toarray()\n","\n","    return tfidf_features\n","\n","# Extract features for training, validation, and testing sets\n","train_features = extract_features(train_df)\n","val_features = extract_features(val_df)\n","test_features = extract_features(test_df)\n","\n","# Save extracted features\n","pd.DataFrame(train_features).to_csv(\"train_features.csv\", index=False, header=False)\n","pd.DataFrame(val_features).to_csv(\"val_features.csv\", index=False, header=False)\n","pd.DataFrame(test_features).to_csv(\"test_features.csv\", index=False, header=False)\n"]},{"cell_type":"code","execution_count":3,"id":"2fda15bc","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321,"referenced_widgets":["7f0479f4cfc74f95b7c483fd5d3e80a0","5db202ea60054271aadf3d516dc993b3","4459c30c3ac9451796442e1bfacd7ccb","9f20a4282f1945799faed632c232c30b","3985ab0edf3a4fb18ea70ca45bad018f","4029128b551845bcb37fad488540084a","ac5f18537ca34e9e94d90c1538073031","ac13c9b90b144369925dea6a9c418b20","47867e89782c403889a4807551258333","35e6452dfe9843578b1dbd1ea6327141","045b39502bf54c68b2cd83b6a56f9b45","ebaa7c40236143149c116ac70320b35f","d1257cf00ed548da8e72682a36d86652","cdd5dc01bd9e4598b92ae590d01042ca","bb4db883347346279317f0375c8ead68","21a5318fe6984800abc799cb78c33e9e","54a7f80d14594a9dbbb8d029fbf37f3d","a24bc46de6744259b7e463eb75a2f401","6004f54aa531479fb4c6c73de1fe65ca","91b66502aae64791946b201eb700eaa7","53d3b1dd00f84a64b95ab176b77034c8","dbe5d43756884d51acf1b7b2d1cf2f21","8888804bd1f2446280c0fe219b4324cc","d21e4e06ec2141b8acf1bf104085a002","9bdcf9ed0d384a7992abe17def2c1464","42e425b8261c4e6e8be7aa52c47a8552","8a0c074ead3149f9a4484350ccefaa35","ebab80f77b7743719e254862a5af512f","ac6f76b8746b41b0926ba692af1ee66d","dd9d963d1ac347288c963b14d2bb7540","6830d74353d342a8a1aa800e2774cb6c","b5bca420227d47ef8c9afce1a8a57b23","88c058ad404345caba9588e85a9417ec","7ad2c104bc184bd8a8c23ed2d21e5cc7","3d62e49493664123a9ec219db53a1661","dceafedcdc1246378640e3795fbef913","d82bd54788d648b78d59a7c8be63b95d","265fc79c4bb04df3a1337231e50fd037","9770b4e4f139441283a36eefd5bd4d17","dd8ffb199fb94f8eae39cad451b39608","d705563aa3614d8591cd2f02ab643430","bf2f59862946469991454de1bc77c64c","bc87ba82ffff455bbc2fd836f1446b73","80e4cec964d847aa9c87926b97dc5870"]},"id":"2fda15bc","executionInfo":{"status":"ok","timestamp":1713079499650,"user_tz":-330,"elapsed":13639,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}},"outputId":"65e161df-6828-4b45-b6cd-486832169f35"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f0479f4cfc74f95b7c483fd5d3e80a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebaa7c40236143149c116ac70320b35f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8888804bd1f2446280c0fe219b4324cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ad2c104bc184bd8a8c23ed2d21e5cc7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Train Tokenized Shape: torch.Size([15989, 26])\n","Validation Tokenized Shape: torch.Size([1997, 23])\n","Test Tokenized Shape: torch.Size([2000, 20])\n"]}],"source":["import pandas as pd\n","from transformers import BertTokenizer\n","\n","# Load preprocessed dataset\n","train_df = pd.read_csv(\"train_dataset.csv\")\n","val_df = pd.read_csv(\"val_dataset.csv\")\n","test_df = pd.read_csv(\"test_dataset.csv\")\n","\n","# Initialize BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def tokenize_text(df):\n","    # Drop rows with missing or empty 'processed_text'\n","    df = df.dropna(subset=['processed_text'])\n","    df = df[df['processed_text'] != '']\n","\n","    # Tokenize the text data\n","    tokenized_data = tokenizer(df['processed_text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n","\n","    return tokenized_data\n","\n","train_tokenized = tokenize_text(train_df)\n","val_tokenized = tokenize_text(val_df)\n","test_tokenized = tokenize_text(test_df)\n","\n","# Print tokenized data shapes\n","print(\"Train Tokenized Shape:\", train_tokenized['input_ids'].shape)\n","print(\"Validation Tokenized Shape:\", val_tokenized['input_ids'].shape)\n","print(\"Test Tokenized Shape:\", test_tokenized['input_ids'].shape)\n"]},{"cell_type":"code","execution_count":4,"id":"260bb584","metadata":{"id":"260bb584","executionInfo":{"status":"ok","timestamp":1713079502706,"user_tz":-330,"elapsed":518,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import BertModel\n","\n","class BertClassifier(nn.Module):\n","    def __init__(self, num_classes):\n","        super(BertClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.dropout = nn.Dropout(0.1)  # Adjust dropout probability as needed\n","        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.fc(pooled_output)\n","        return logits\n"]},{"cell_type":"code","execution_count":5,"id":"0ab666c9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ab666c9","executionInfo":{"status":"ok","timestamp":1713079506087,"user_tz":-330,"elapsed":518,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}},"outputId":"6fc3ad96-9d9e-4de9-8458-39e07931c4fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input IDs Shape: torch.Size([15989, 26])\n","Attention Mask Shape: torch.Size([15989, 26])\n","Labels Shape: torch.Size([15998])\n"]}],"source":["print(\"Input IDs Shape:\", train_tokenized['input_ids'].shape)\n","print(\"Attention Mask Shape:\", train_tokenized['attention_mask'].shape)\n","print(\"Labels Shape:\", torch.tensor(train_df['label'].tolist()).shape)"]},{"cell_type":"code","execution_count":6,"id":"2f882505","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2f882505","executionInfo":{"status":"ok","timestamp":1713079517967,"user_tz":-330,"elapsed":8813,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}},"outputId":"48f3eb96-b081-4083-c873-96b17b4769bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample of train_df:\n","                                                text  label  \\\n","0                 I'll write something about myself.      1   \n","1      To use English well, I register on this site.      1   \n","2  I don't know how much money it would cost, or ...      1   \n","3                 because it is a very good website.      1   \n","4  KanJi is japanese character and have variety o...      0   \n","\n","                                      processed_text  \n","0                                ill write something  \n","1                     use english well register site  \n","2  dont know much money would cost much time woul...  \n","3                                       good website  \n","4           kanji japanese character variety meaning  \n","\n","Sample of val_df:\n","                                                text  label  \\\n","0                               I'm college student.      0   \n","1                             be hired as consultant      0   \n","2                                It’s sunny morning.      0   \n","3         I was looking forward to these few months.      1   \n","4  I asked class teachers to plant seeds in the b...      0   \n","\n","                             processed_text  \n","0                        im college student  \n","1                          hired consultant  \n","2                           ’ sunny morning  \n","3                     looking forward month  \n","4  asked class teacher plant seed beginning  \n","\n","Sample of test_df:\n","                                                text  label  \\\n","0            2-[His mother words touched his heart].      0   \n","1                                         many time.      0   \n","2                    Sometime l want change my work.      0   \n","3  I think that I may hold a fine if there is a p...      0   \n","4  I get lazy, which is the most serious problem ...      1   \n","\n","                            processed_text  \n","0           2his mother word touched heart  \n","1                                many time  \n","2              sometime l want change work  \n","3  think may hold fine person able protect  \n","4            get lazy serious problem mine  \n"]}],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import BertTokenizer\n","\n","# Assuming train_df, val_df, and test_df are your DataFrames containing the data\n","# Initialize BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to tokenize text data\n","def tokenize_text(df, tokenizer):\n","    tokenized_data = tokenizer(df['text'].tolist(), padding=True, truncation=True, return_tensors='pt', max_length=128, return_attention_mask=True)\n","    return tokenized_data\n","\n","# Load the data into DataFrames using the correct file paths\n","train_df = pd.read_csv('train_dataset.csv')\n","val_df = pd.read_csv('val_dataset.csv')\n","test_df = pd.read_csv('test_dataset.csv')\n","\n","\n","# Check the first few rows of each DataFrame\n","print(\"Sample of train_df:\")\n","print(train_df.head())\n","\n","print(\"\\nSample of val_df:\")\n","print(val_df.head())\n","\n","print(\"\\nSample of test_df:\")\n","print(test_df.head())\n","# Tokenize text data for train, validation, and test sets\n","train_tokenized = tokenize_text(train_df, tokenizer)\n","val_tokenized = tokenize_text(val_df, tokenizer)\n","test_tokenized = tokenize_text(test_df, tokenizer)\n","\n","# Filter the labels to match the number of input samples\n","filtered_labels_train = train_df['label'][:len(train_tokenized['input_ids'])]\n","filtered_labels_val = val_df['label'][:len(val_tokenized['input_ids'])]\n","filtered_labels_test = test_df['label'][:len(test_tokenized['input_ids'])]\n","\n","# Create TensorDataset for train, validation, and test sets\n","train_dataset = TensorDataset(train_tokenized['input_ids'], train_tokenized['attention_mask'], torch.tensor(filtered_labels_train.tolist()))\n","val_dataset = TensorDataset(val_tokenized['input_ids'], val_tokenized['attention_mask'], torch.tensor(filtered_labels_val.tolist()))\n","test_dataset = TensorDataset(test_tokenized['input_ids'], test_tokenized['attention_mask'], torch.tensor(filtered_labels_test.tolist()))\n","\n","# Create DataLoader for train, validation, and test sets\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":7,"id":"1d47de7d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1d47de7d","executionInfo":{"status":"ok","timestamp":1713079526880,"user_tz":-330,"elapsed":5430,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}},"outputId":"3ba86c5a-d594-4b0e-807b-5fb56b62202f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Length of Train Dataset: 15998\n","Length of Validation Dataset: 2000\n"]}],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import BertTokenizer\n","\n","# Initialize BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to tokenize text data\n","def tokenize_text(df, tokenizer):\n","    tokenized_data = tokenizer(df['text'].tolist(), padding=True, truncation=True, return_tensors='pt', max_length=128, return_attention_mask=True)\n","    return tokenized_data\n","\n","# Tokenize text data for train and validation sets\n","train_tokenized = tokenize_text(train_df, tokenizer)\n","val_tokenized = tokenize_text(val_df, tokenizer)\n","\n","# Filter the labels to match the number of input samples\n","filtered_labels_train = train_df['label'][:len(train_tokenized['input_ids'])]\n","filtered_labels_val = val_df['label'][:len(val_tokenized['input_ids'])]\n","\n","# Create TensorDataset for train and validation sets\n","train_dataset = TensorDataset(train_tokenized['input_ids'], train_tokenized['attention_mask'], torch.tensor(filtered_labels_train.tolist()))\n","val_dataset = TensorDataset(val_tokenized['input_ids'], val_tokenized['attention_mask'], torch.tensor(filtered_labels_val.tolist()))\n","\n","# Create DataLoader for train and validation sets\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Check the lengths of the datasets\n","print(\"Length of Train Dataset:\", len(train_dataset))\n","print(\"Length of Validation Dataset:\", len(val_dataset))\n"]},{"cell_type":"code","execution_count":8,"id":"802d2a6b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208,"referenced_widgets":["579c28d27f4c439ba1211bb889c69564","ae85730fa6514100b9e4dc129e736109","ebc53f83e30f4f828b53b4efcd9ca4d6","d38b020d8e9a4448894b3fe22ccf79bb","9bbd4ac8990644219c9da6a74795d610","1d13ce16dec7485e9b5344fa42be84e1","b12b679e83384af1a6c52f94e3674836","e650e3e7093f47f58f47c76cfe12d987","6ec79ff88ec247c8bd3036f6cb3339b5","ed9bdebce9be425f88f3550011a5d63e","951d9da9243649308a3a1fbeba9413d4"]},"id":"802d2a6b","executionInfo":{"status":"ok","timestamp":1713079908936,"user_tz":-330,"elapsed":379602,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}},"outputId":"48186979-1c4a-4c71-8824-ccdc952d905a"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"579c28d27f4c439ba1211bb889c69564"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Training Loss: 0.4483939927741885,  Validation Accuracy: 80.35%\n","Epoch 2/3, Training Loss: 0.24040161559917034,  Validation Accuracy: 81.89999999999999%\n","Epoch 3/3, Training Loss: 0.10999372425326147,  Validation Accuracy: 84.05%\n"]}],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","\n","# Check for GPU availability\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU is not available, using CPU instead\")\n","\n","# Initialize BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to tokenize text data\n","def tokenize_text(df, tokenizer):\n","    tokenized_data = tokenizer(df['text'].tolist(), padding=True, truncation=True, return_tensors='pt', max_length=128, return_attention_mask=True)\n","    return tokenized_data\n","\n","# Load the data into DataFrames using the correct file paths\n","train_df = pd.read_csv('train_dataset.csv')\n","val_df = pd.read_csv('val_dataset.csv')\n","test_df = pd.read_csv('test_dataset.csv')\n","\n","# Tokenize text data for train, validation, and test sets\n","train_tokenized = tokenize_text(train_df, tokenizer)\n","val_tokenized = tokenize_text(val_df, tokenizer)\n","test_tokenized = tokenize_text(test_df, tokenizer)\n","\n","# Filter the labels to match the number of input samples\n","filtered_labels_train = train_df['label'][:len(train_tokenized['input_ids'])]\n","filtered_labels_val = val_df['label'][:len(val_tokenized['input_ids'])]\n","filtered_labels_test = test_df['label'][:len(test_tokenized['input_ids'])]\n","\n","# Create TensorDataset for train, validation, and test sets\n","train_dataset = TensorDataset(train_tokenized['input_ids'], train_tokenized['attention_mask'], torch.tensor(filtered_labels_train.tolist()))\n","val_dataset = TensorDataset(val_tokenized['input_ids'], val_tokenized['attention_mask'], torch.tensor(filtered_labels_val.tolist()))\n","test_dataset = TensorDataset(test_tokenized['input_ids'], test_tokenized['attention_mask'], torch.tensor(filtered_labels_test.tolist()))\n","\n","# Create DataLoader for train, validation, and test sets\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","# Initialize BERT model for sequence classification\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n","\n","# Define the optimizer and loss function\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# Training loop\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for batch in train_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    # Evaluate the model on the validation set\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs.logits, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)},  Validation Accuracy: {(correct/total)*100}%\")\n","\n","\n","# Save the trained model\n","torch.save(model.state_dict(), 'bert_model.pth')"]},{"cell_type":"code","source":["import torch\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Function to evaluate the model\n","def evaluate_model(model, dataloader):\n","\n","    model.eval()  # Set the model to evaluation mode\n","    all_predictions = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            _, predicted = torch.max(outputs.logits, 1)\n","\n","            # Convert tensors to numpy arrays and extend lists\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    return all_predictions, all_labels\n","\n","# Evaluate the model on the validation set\n","val_predictions, val_labels = evaluate_model(model, val_loader)\n","\n","# Calculate classification report\n","classification_rep = classification_report(val_labels, val_predictions)\n","print(\"Classification Report for Validation Set:\")\n","print(classification_rep)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(val_labels, val_predictions)\n","print(\"Accuracy on Validation Set:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qGB-XpL221jQ","executionInfo":{"status":"ok","timestamp":1713079922751,"user_tz":-330,"elapsed":3783,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}},"outputId":"c42ab882-2658-411d-8e6e-85c79845f42d"},"id":"qGB-XpL221jQ","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification Report for Validation Set:\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.82      0.84      1024\n","           1       0.82      0.86      0.84       976\n","\n","    accuracy                           0.84      2000\n","   macro avg       0.84      0.84      0.84      2000\n","weighted avg       0.84      0.84      0.84      2000\n","\n","Accuracy on Validation Set: 0.8405\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n","from torch.utils.data import DataLoader\n","\n","def classify_text_from_excel(input_file, output_file, model, tokenizer, device):\n","    # Read input data from Excel file\n","    df = pd.read_excel(input_file)\n","\n","    # Select only the first 1000 rows\n","    df = df.head(1000)\n","\n","    # Convert text column to a list of strings\n","    text_list = df['text'].astype(str).tolist()\n","\n","    # Process text inputs\n","    tokenized_data = tokenizer(text_list, padding=True, truncation=True, return_tensors='pt', max_length=128, return_attention_mask=True)\n","    input_ids = tokenized_data['input_ids'].to(device)\n","    attention_mask = tokenized_data['attention_mask'].to(device)\n","\n","    # Move model to device\n","    model.to(device)\n","\n","    # Predict labels using the model\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","    predictions = torch.argmax(outputs.logits, axis=1).cpu().numpy()\n","\n","    # Add predictions to DataFrame\n","    df['predicted_label'] = predictions\n","\n","    # Save DataFrame with predictions back to CSV file\n","    df.to_csv(output_file, index=False)\n","\n","# Initialize tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Input and output filenames\n","input_filename = '/content/sample_data/test_data.xlsx'\n","output_filename = 'output_file_only_labeled.csv'\n","\n","# Call the function to classify text from Excel\n","classify_text_from_excel(input_filename, output_filename, model, tokenizer, device)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJHXKvhuXuDP","executionInfo":{"status":"ok","timestamp":1713080384699,"user_tz":-330,"elapsed":4936,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}},"outputId":"1f2ab349-4e63-46a2-cf9a-f28aaa750481"},"id":"LJHXKvhuXuDP","execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","execution_count":13,"id":"76d8a769","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76d8a769","executionInfo":{"status":"ok","timestamp":1713080046543,"user_tz":-330,"elapsed":27024,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}},"outputId":"da6684e3-30f9-4b48-8015-65082c26de93"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Enter your sentence: Jeff ran a mile and drops his keys.\n","The sentence contains grammar errors.\n"]}],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# Initialize BERT tokenizer and load pretrained model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","model.load_state_dict(torch.load('bert_model.pth'))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","# Function to validate user sentence\n","def validate_sentence(sentence):\n","    # Tokenize the input sentence\n","    inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt').to(device)\n","\n","    # Pass input through the model\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Interpret predictions\n","    prediction = torch.argmax(outputs.logits).item()\n","    if prediction == 1:\n","        print(\"The sentence is grammatically correct.\")\n","    else:\n","        print(\"The sentence contains grammar errors.\")\n","\n","# Get user input\n","user_sentence = input(\"Enter your sentence: \")\n","\n","# Validate the user sentence\n","validate_sentence(user_sentence)\n"]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n","from torch.utils.data import DataLoader\n","from nltk import pos_tag, word_tokenize\n","\n","# Download NLTK data\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Function to validate sentence for grammatical errors\n","def validate_sentence(sentence):\n","    # Ensure that the input is a string\n","    if not isinstance(sentence, str):\n","        return 1, \"None\"  # No error\n","\n","    # Tokenize the input sentence\n","    tokens = word_tokenize(sentence)\n","\n","    # Get Part-of-Speech tags for the tokens\n","    pos_tags = pos_tag(tokens)\n","\n","    # Check for grammatical errors\n","    error_type = None\n","    for i in range(len(pos_tags)-1):\n","        # Rule 1: Subject-Verb Agreement\n","        if pos_tags[i][1].startswith('N') and pos_tags[i+1][1].startswith('VB'):\n","            error_type = \"Subject-Verb Agreement\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 2: Present Simple Tense\n","        if pos_tags[i][1] == 'VBP' and pos_tags[i][0].endswith('s'):\n","            error_type = \"Present Simple Tense\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 3: Present Continuous Tense\n","        if pos_tags[i][1] == 'VBG' and pos_tags[i][0] != 'am':\n","            error_type = \"Present Continuous Tense\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 4: Present Perfect Tense\n","        if pos_tags[i][1] == 'VBN' and pos_tags[i][0] != 'been':\n","            error_type = \"Present Perfect Tense\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 5: Present Perfect Continuous Tense\n","        if pos_tags[i][1] == 'VBG' and pos_tags[i][0] == 'been':\n","            error_type = \"Present Perfect Continuous Tense\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 6: Past Simple Tense\n","        if pos_tags[i][1] == 'VBD' and not pos_tags[i][0].endswith('ed'):\n","            error_type = \"Past Simple Tense\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 7: Past Continuous Tense\n","        if pos_tags[i][1] == 'VBD' and pos_tags[i][0] == 'were':\n","            error_type = \"Past Continuous Tense\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 8: Past Perfect Tense\n","        if pos_tags[i][1] == 'VBN' and pos_tags[i][0] != 'had':\n","            error_type = \"Past Perfect Tense\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 9: Past Perfect Continuous Tense\n","        if pos_tags[i][1] == 'VBN' and pos_tags[i][0] == 'had':\n","            error_type = \"Past Perfect Continuous Tense\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 10: Future Simple Tense\n","        if pos_tags[i][1] == 'MD' and pos_tags[i][0] not in ['will', 'shall']:\n","            error_type = \"Future Simple Tense\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 11: Future Continuous Tense\n","        if pos_tags[i][1] == 'MD' and pos_tags[i][0] in ['will', 'shall']:\n","            error_type = \"Future Continuous Tense\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 12: Future Perfect Tense\n","        if pos_tags[i][1] == 'MD' and pos_tags[i][0] in ['will', 'shall']:\n","            error_type = \"Future Perfect Tense\"\n","            return 0, error_type  # Error found\n","\n","        # Rule 13: Future Perfect Continuous Tense\n","        if pos_tags[i][1] == 'MD' and pos_tags[i][0] in ['will', 'shall']:\n","            error_type = \"Future Perfect Continuous Tense\"\n","            return 0, error_type  # Error found\n","\n","    return 1, \"None\"  # No error\n","\n","# Function to classify text from Excel file\n","def classify_text_from_excel(input_file, output_file, model, tokenizer, device):\n","    # Read input data from Excel file\n","    df = pd.read_excel(input_file)\n","\n","\n","    df = df.sample(n=1000, random_state=42)\n","\n","    # Convert text column to a list of strings\n","    text_list = df['text'].astype(str).tolist()\n","\n","    # Process text inputs\n","    tokenized_data = tokenizer(text_list, padding=True, truncation=True, return_tensors='pt', max_length=128, return_attention_mask=True)\n","    input_ids = tokenized_data['input_ids'].to(device)\n","    attention_mask = tokenized_data['attention_mask'].to(device)\n","\n","    # Move model to device\n","    model.to(device)\n","\n","    # Predict labels using the model\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","    predictions = torch.argmax(outputs.logits, axis=1).cpu().numpy()\n","\n","    # Add columns for grammatical error detection and error type\n","    df['label'], df['error_type'] = zip(*df['text'].apply(validate_sentence))\n","\n","    # Save DataFrame with predictions, error detection, and error type back to CSV file\n","    df.to_csv(output_file, index=False)\n","\n","# Initialize tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Input and output filenames\n","input_filename = '/content/sample_data/test_data.xlsx'\n","output_filename = 'output_file.csv'\n","\n","# Call the function to classify text from Excel\n","classify_text_from_excel(input_filename, output_filename, model, tokenizer, device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ZBx8poYfpQe","executionInfo":{"status":"ok","timestamp":1713082090036,"user_tz":-330,"elapsed":3152,"user":{"displayName":"Movie verse","userId":"02923746787528582892"}},"outputId":"03f9e0fa-6e4d-4310-8168-95c72ef939f3"},"id":"8ZBx8poYfpQe","execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"colab":{"provenance":[{"file_id":"1Ri3tIdvrehY7MpusZ35zkiNzqlpPzV6-","timestamp":1712904347206}],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7f0479f4cfc74f95b7c483fd5d3e80a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5db202ea60054271aadf3d516dc993b3","IPY_MODEL_4459c30c3ac9451796442e1bfacd7ccb","IPY_MODEL_9f20a4282f1945799faed632c232c30b"],"layout":"IPY_MODEL_3985ab0edf3a4fb18ea70ca45bad018f"}},"5db202ea60054271aadf3d516dc993b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4029128b551845bcb37fad488540084a","placeholder":"​","style":"IPY_MODEL_ac5f18537ca34e9e94d90c1538073031","value":"tokenizer_config.json: 100%"}},"4459c30c3ac9451796442e1bfacd7ccb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac13c9b90b144369925dea6a9c418b20","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_47867e89782c403889a4807551258333","value":48}},"9f20a4282f1945799faed632c232c30b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35e6452dfe9843578b1dbd1ea6327141","placeholder":"​","style":"IPY_MODEL_045b39502bf54c68b2cd83b6a56f9b45","value":" 48.0/48.0 [00:00&lt;00:00, 1.73kB/s]"}},"3985ab0edf3a4fb18ea70ca45bad018f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4029128b551845bcb37fad488540084a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac5f18537ca34e9e94d90c1538073031":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac13c9b90b144369925dea6a9c418b20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47867e89782c403889a4807551258333":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35e6452dfe9843578b1dbd1ea6327141":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"045b39502bf54c68b2cd83b6a56f9b45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebaa7c40236143149c116ac70320b35f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1257cf00ed548da8e72682a36d86652","IPY_MODEL_cdd5dc01bd9e4598b92ae590d01042ca","IPY_MODEL_bb4db883347346279317f0375c8ead68"],"layout":"IPY_MODEL_21a5318fe6984800abc799cb78c33e9e"}},"d1257cf00ed548da8e72682a36d86652":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54a7f80d14594a9dbbb8d029fbf37f3d","placeholder":"​","style":"IPY_MODEL_a24bc46de6744259b7e463eb75a2f401","value":"vocab.txt: 100%"}},"cdd5dc01bd9e4598b92ae590d01042ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6004f54aa531479fb4c6c73de1fe65ca","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_91b66502aae64791946b201eb700eaa7","value":231508}},"bb4db883347346279317f0375c8ead68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53d3b1dd00f84a64b95ab176b77034c8","placeholder":"​","style":"IPY_MODEL_dbe5d43756884d51acf1b7b2d1cf2f21","value":" 232k/232k [00:00&lt;00:00, 3.85MB/s]"}},"21a5318fe6984800abc799cb78c33e9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54a7f80d14594a9dbbb8d029fbf37f3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a24bc46de6744259b7e463eb75a2f401":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6004f54aa531479fb4c6c73de1fe65ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91b66502aae64791946b201eb700eaa7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53d3b1dd00f84a64b95ab176b77034c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbe5d43756884d51acf1b7b2d1cf2f21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8888804bd1f2446280c0fe219b4324cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d21e4e06ec2141b8acf1bf104085a002","IPY_MODEL_9bdcf9ed0d384a7992abe17def2c1464","IPY_MODEL_42e425b8261c4e6e8be7aa52c47a8552"],"layout":"IPY_MODEL_8a0c074ead3149f9a4484350ccefaa35"}},"d21e4e06ec2141b8acf1bf104085a002":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebab80f77b7743719e254862a5af512f","placeholder":"​","style":"IPY_MODEL_ac6f76b8746b41b0926ba692af1ee66d","value":"tokenizer.json: 100%"}},"9bdcf9ed0d384a7992abe17def2c1464":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd9d963d1ac347288c963b14d2bb7540","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6830d74353d342a8a1aa800e2774cb6c","value":466062}},"42e425b8261c4e6e8be7aa52c47a8552":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5bca420227d47ef8c9afce1a8a57b23","placeholder":"​","style":"IPY_MODEL_88c058ad404345caba9588e85a9417ec","value":" 466k/466k [00:00&lt;00:00, 19.1MB/s]"}},"8a0c074ead3149f9a4484350ccefaa35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebab80f77b7743719e254862a5af512f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac6f76b8746b41b0926ba692af1ee66d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd9d963d1ac347288c963b14d2bb7540":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6830d74353d342a8a1aa800e2774cb6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5bca420227d47ef8c9afce1a8a57b23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88c058ad404345caba9588e85a9417ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ad2c104bc184bd8a8c23ed2d21e5cc7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d62e49493664123a9ec219db53a1661","IPY_MODEL_dceafedcdc1246378640e3795fbef913","IPY_MODEL_d82bd54788d648b78d59a7c8be63b95d"],"layout":"IPY_MODEL_265fc79c4bb04df3a1337231e50fd037"}},"3d62e49493664123a9ec219db53a1661":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9770b4e4f139441283a36eefd5bd4d17","placeholder":"​","style":"IPY_MODEL_dd8ffb199fb94f8eae39cad451b39608","value":"config.json: 100%"}},"dceafedcdc1246378640e3795fbef913":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d705563aa3614d8591cd2f02ab643430","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf2f59862946469991454de1bc77c64c","value":570}},"d82bd54788d648b78d59a7c8be63b95d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc87ba82ffff455bbc2fd836f1446b73","placeholder":"​","style":"IPY_MODEL_80e4cec964d847aa9c87926b97dc5870","value":" 570/570 [00:00&lt;00:00, 15.5kB/s]"}},"265fc79c4bb04df3a1337231e50fd037":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9770b4e4f139441283a36eefd5bd4d17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd8ffb199fb94f8eae39cad451b39608":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d705563aa3614d8591cd2f02ab643430":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf2f59862946469991454de1bc77c64c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc87ba82ffff455bbc2fd836f1446b73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80e4cec964d847aa9c87926b97dc5870":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"579c28d27f4c439ba1211bb889c69564":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae85730fa6514100b9e4dc129e736109","IPY_MODEL_ebc53f83e30f4f828b53b4efcd9ca4d6","IPY_MODEL_d38b020d8e9a4448894b3fe22ccf79bb"],"layout":"IPY_MODEL_9bbd4ac8990644219c9da6a74795d610"}},"ae85730fa6514100b9e4dc129e736109":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d13ce16dec7485e9b5344fa42be84e1","placeholder":"​","style":"IPY_MODEL_b12b679e83384af1a6c52f94e3674836","value":"model.safetensors: 100%"}},"ebc53f83e30f4f828b53b4efcd9ca4d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e650e3e7093f47f58f47c76cfe12d987","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ec79ff88ec247c8bd3036f6cb3339b5","value":440449768}},"d38b020d8e9a4448894b3fe22ccf79bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed9bdebce9be425f88f3550011a5d63e","placeholder":"​","style":"IPY_MODEL_951d9da9243649308a3a1fbeba9413d4","value":" 440M/440M [00:02&lt;00:00, 186MB/s]"}},"9bbd4ac8990644219c9da6a74795d610":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d13ce16dec7485e9b5344fa42be84e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b12b679e83384af1a6c52f94e3674836":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e650e3e7093f47f58f47c76cfe12d987":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ec79ff88ec247c8bd3036f6cb3339b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed9bdebce9be425f88f3550011a5d63e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"951d9da9243649308a3a1fbeba9413d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}